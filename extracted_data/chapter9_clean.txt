9

Support Vector Machines

In this chapter, we discuss the support vector machine(SVM),an approach forclassificationthatwasdevelopedinthecomputersciencecommunityin the 1990 s and that has grown in popularity since then. SVMs have been shown to perform well in a variety of settings, and are often considered one of the best “out of the box” classifiers.

The support vector machine is a generalization of a simple and intuitive classifier called the maximal margin classifier, which we introduce in

Section 9.1. Though it is elegant and simple, we will see that this classifier unfortunately cannot be applied to most data sets, since it requires that the classes be separable by a linear boundary. In Section 9.2, we introduce the support vector classifier, an extension of the maximal margin classifier that can be applied in a broader range of cases. Section 9.3 introduces the support vector machine, which is a further extension of the support vector classifier in order to accommodatenon-linear class boundaries.Support vector machines are intended for the binary classification setting in which there are two classes; in Section 9.4 we discuss extensions of support vector machines to the case of more than two classes. In Section 9.5 we discuss the close connections between support vector machines and other statistical methods such as logistic regression.

People often loosely refer to the maximal margin classifier, the support vector classifier, and the support vector machine as “support vector machines”. To avoid confusion, we will carefully distinguish between these three notions in this chapter.

9.1 Maximal Margin Classifier

In this section, we define a hyperplane and introduce the concept of an optimal separating hyperplane.

© Springer Nature Switzerland AG 2023 367

G. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics, https://doi.org/10.1007/978-3-031-38747-0_9

368 9. Support Vector Machines

9.1.1 What Is a Hyperplane?

In a p-dimensional space, a hyperplane is a flat affine subspace of hyperplane dimension p 1.1 For instance, in two dimensions, a hyperplane is a flat

− one-dimensional subspace—in other words, a line. In three dimensions, a hyperplane is a flat two-dimensional subspace—that is, a plane. In p > 3 dimensions, it can be hard to visualize a hyperplane, but the notion of a

(p 1)-dimensional flat subspace still applies.

−

The mathematical definition of a hyperplane is quite simple. In two di- mensions, a hyperplane is defined by the equation

β +β X +β X =0 (9.1)

0 1 1 2 2 forparametersβ ,β ,andβ .Whenwesaythat(9.1)“defines”thehyper-

0 1 2 plane, we mean that any X = (X ,X )T for which (9.1) holds is a point

1 2 on the hyperplane. Note that (9.1) is simply the equation of a line, since indeed in two dimensions a hyperplane is a line.

Equation 9.1 can be easily extended to the p-dimensional setting:

β +β X +β X + +β X =0 (9.2)

0 1 1 2 2 p p

··· defines a p-dimensional hyperplane, again in the sense that if a point X =

(X ,X ,...,X )T inp-dimensionalspace(i.e.avectoroflengthp)satisfies

1 2 p

(9.2), then X lies on the hyperplane.

Now, suppose that X does not satisfy (9.2); rather,

β +β X +β X + +β X >0. (9.3)

0 1 1 2 2 p p

···

Then this tells us that X lies to one side of the hyperplane. On the other hand, if

β +β X +β X + +β X <0, (9.4)

0 1 1 2 2 p p

··· then X lies on the other side of the hyperplane. So we can think of the hyperplaneasdividingp-dimensionalspaceintotwohalves.Onecaneasily determine on which side of the hyper plane a point lies by simply calculating the sign of the left-hand side of (9.2). A hyperplane in two-dimensional space is shown in Figure 9.1.

9.1.2 Classification Using a Separating Hyperplane

Now suppose that we have an n p data matrix X that consists of n

× training observations in p-dimensional space, x x

11 n 1

. .

x = . ,...,x = . , (9.5)

1  .  n  .  x x

 1 p  np

    and that these observations fall into two classes—that is, y ,...,y

1 n

∈

1,1 where 1 representsoneclassand 1 theotherclass.Wealsohavea

{− } −

1Thewordaffineindicatesthatthesubspaceneednotpassthroughtheorigin.

9.1 Maximal Margin Classifier 369

−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5

5.1

0.1

5.0

0.0

5.0−

0.1−

5.1−

X

1

X 2

FIGURE 9.1. The hyperplane 1+2X 1 +3X 2 =0 is shown. The blue region is the set of points for which 1+2X 1 +3X 2 >0, and the purple region is the set of points for which 1+2X 1 +3X 2 <0.

T testobservation,ap-vector of observed featuresx ∗ = x ∗1 ... x ∗p .Our goal is to develop a classifier based on the training data that will correctly

’ ( classify the test observation using its feature measurements. We have seen a number of approaches for this task, such as linear discriminant analysis and logistic regression in Chapter 4, and classification trees, bagging, and boostingin Chapter 8.We will now see a new approach that is based upon the concept of a separating hyperplane.

separating

Suppose that it is possible to construct a hyperplane that separates the hyperplane training observations perfectly according to their class labels. Examples of three such separating hyperplanes are shown in the left-hand panel of

Figure 9.2.Wecanlabeltheobservationsfromtheblueclassasy =1 and i those from the purple class as y = 1. Then a separating hyperplane has i

− the property that

β +β x +β x + +β x >0 if y =1, (9.6)

0 1 i 1 2 i 2 p ip i

··· and

β +β x +β x + +β x <0 if y = 1. (9.7)

0 1 i 1 2 i 2 p ip i

··· −

Equivalently, a separating hyperplane has the property that y (β +β x +β x + +β x )>0 (9.8) i 0 1 i 1 2 i 2 p ip

··· for all i=1,...,n.

Ifaseparatinghyperplaneexists,wecanuseittoconstructaverynatural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located. The right-hand panel of Figure 9.2 shows an example of such a classifier. That is, we classify the test observation x

∗ basedonthesignoff(x )=β +β x +β x + +β x .Iff(x )ispositive,

∗ 0 1 ∗1 2 ∗2

··· p ∗p ∗ thenweassignthetestobservationtoclass 1,andiff(x )isnegative,then

∗ weassignittoclass 1.Wecanalsomakeuseofthemagnitudeoff(x ).If

∗

−

370 9. Support Vector Machines

−1 0 1 2 3

3

2

1

0

1−

−1 0 1 2 3

3

2

1

0

1−

X X

1 1

X 2 X 2

FIGURE 9.2. Left: There are two classes of observations, shown in blue and in purple, each of which has measurements on two variables. Three separating hyperplanes, out of many possible, are shown in black. Right: A separating hy- perplane is shown in black. The blue and purple grid indicates the decision rule made by a classifier based on this separating hyperplane: a test observation that falls in the blue portion of the grid will be assigned to the blue class, and a test observation that falls into the purple portion of the grid will be assigned to the purple class.

f(x )isfarfromzero,thenthismeansthatx liesfarfromthehyperplane,

∗ ∗ andsowecanbeconfidentaboutourclassassignmentforx .Ontheother

∗ hand,iff(x )isclosetozero,thenx islocatednearthehyperplane,andso

∗ ∗ wearelesscertainabouttheclass assignmentforx .Notsurprisingly,and

∗ asweseein Figure 9.2,aclassifierthatisbasedonaseparatinghyperplane leads to a linear decision boundary.

9.1.3 The Maximal Margin Classifier

In general, if our data can be perfectly separated using a hyperplane, then there will in fact exist an infinite number of such hyperplanes. This is becauseagivenseparatinghyperplanecanusuallybeshiftedatinybitupor down,orrotated,withoutcomingintocontactwithanyoftheobservations.

Three possible separating hyperplanes are shown in the left-hand panel of Figure 9.2. In order to construct a classifier based upon a separating hyperplane, we must have a reasonable way to decide which of the infinite possible separating hyperplanes to use.

A natural choice is the maximal margin hyperplane (also known as the maximal optimal separating hyperplane), which is the separating hyperplane that margin is farthest from the training observations. That is, we can compute the hyperplane

(perpendicular)distancefromeachtrainingobservationtoagivenseparat- optimal inghyperplane;thesmallestsuchdistanceistheminimaldistancefromthe separating observations to the hyperplane, and is known as the margin. The maximal hyperplane margin hyperplane is the separating hyperplane for which the margin is margin largest—that is, it is the hyperplane that has the farthest minimum dis- tance to the training observations. We can then classify a test observation basedonwhichsideofthemaximalmarginhyperplaneitlies.Thisisknown

9.1 Maximal Margin Classifier 371

−1 0 1 2 3

3

2

1

0

1−

X

1

X 2

FIGURE 9.3. There are two classes of observations, shown in blue and in purple. The maximal margin hyperplane is shown as a solid line. The margin is the distance from the solid line to either of the dashed lines. The two blue points and the purple point that lie on the dashed lines are the support vectors, and the distance from those points to the hyperplane is indicated by arrows. The purple and blue grid indicates the decision rule made by a classifier based on this separating hyperplane.

asthemaximal margin classifier.Wehopethataclassifierthathasalarge maximal margin on the training data will also have a large margin on the test data, margin and hence will classify the test observations correctly. Although the maxi- classifier malmargin classifier is often successful,it can also lead to overfitting when p is large.

If β ,β ,...,β are the coefficients of the maximal margin hyperplane,

0 1 p then the maximal margin classifier classifies the test observation x based

∗ on the sign of f(x )=β +β x +β x + +β x .

∗ 0 1 ∗1 2 ∗2

··· p ∗p

Figure 9.3 shows the maximal margin hyperplane on the data set of

Figure 9.2. Comparing the right-hand panel of Figure 9.2 to Figure 9.3, we see that the maximal margin hyperplane shown in Figure 9.3 does in- deedresultinagreaterminimaldistancebetweentheobservationsandthe separating hyperplane—that is, a larger margin. In a sense, the maximal marginhyperplanerepresentsthemid-lineofthewidest“slab”thatwecan insert between the two classes.

Examining Figure 9.3,weseethatthreetrainingobservationsareequidis- tant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These three observations are known as support vectors,since they are vectorsinp-dimensional space(in Figure 9.3, support p = 2) and they “support” the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyper- plane would move as well. Interestingly, the maximal margin hyperplane dependsdirectlyonthesupportvectors,butnotontheotherobservations: a movement to any of the other observations would not affect the separating hyperplane,providedthattheobservation’smovementdoesnotcauseitto

372 9. Support Vector Machines cross the boundary set by the margin. The fact that the maximal margin hyperplane depends directly on only a small subset of the observations is an important property that will arise later in this chapter when we discuss the support vector classifier and support vector machines.

9.1.4 Construction of the Maximal Margin Classifier

We now consider the task of constructing the maximal margin hyperplane based on a set of n training observations x 1 ,...,x n R p and associated

∈ class labels y ,...,y 1,1 . Briefly, the maximal margin hyperplane

1 n

∈ {− } is the solution to the optimization problem maximize M (9.9)

β0,β1,...,βp,M p subject to β2 =1, (9.10) j j=1

0 y (β +β x +β x + +β x ) M i=1,...,n.(9.11) i 0 1 i 1 2 i 2 p ip

··· ≥ ∀

This optimization problem (9.9)–(9.11) is actually simpler than it looks.

First of all, the constraint in (9.11) that y (β +β x +β x + +β x ) M i=1,...,n i 0 1 i 1 2 i 2 p ip

··· ≥ ∀ guarantees that each observation will be on the correct side of the hyper- plane, provided that M is positive. (Actually, for each observation to be onthecorrectsideofthehyperplanewewouldsimplyneedy (β +β x + i 0 1 i 1

β x + +β x )>0,sotheconstraintin(9.11)infactrequiresthateach

2 i 2 p ip

··· observation be on the correct side of the hyperplane, with some cushion, provided that M is positive.)

Second,notethat(9.10)isnotreallyaconstraintonthehyperplane,since if β +β x +β x + +β x = 0 defines a hyperplane, then so does

0 1 i 1 2 i 2 p ip

··· k(β +β x +β x + +β x )=0 foranyk =0.However,(9.10)adds

0 1 i 1 2 i 2 p ip

··· % meaningto(9.11);onecanshowthatwiththisconstrainttheperpendicular distance from the ith observation to the hyperplane is given by y (β +β x +β x + +β x ).

i 0 1 i 1 2 i 2 p ip

···

Therefore, the constraints (9.10) and (9.11) ensure that each observation is on the correct side of the hyperplane and at least a distance M from the hyperplane. Hence, M represents the margin of our hyperplane, and the optimizationproblemchoosesβ ,β ,...,β tomaximize M.Thisisexactly

0 1 p thedefinitionofthemaximalmarginhyperplane!Theproblem(9.9)–(9.11) can be solved efficiently, but details of this optimization are outside of the scope of this book.

9.1.5 The Non-separable Case

The maximal margin classifier is a very natural way to perform classifi- cation, if a separating hyperplane exists. However, as we have hinted, in many cases no separating hyperplane exists, and so there is no maximal

9.2 Support Vector Classifiers 373

0 1 2 3

0.2

5.1

0.1

5.0

0.0

5.0−

0.1−

X

1

X 2

FIGURE 9.4. There are two classes of observations, shown in blue and in purple.Inthiscase,thetwoclassesarenotseparablebyahyperplane,andsothe maximal margin classifier cannot be used.

marginclassifier.Inthiscase,theoptimizationproblem(9.9)–(9.11)hasno solution with M > 0. An example is shown in Figure 9.4. In this case, we cannotexactly separatethetwoclasses.However,as we will see in the next section, we can extend the concept of a separating hyperplane in order to develop a hyperplane that almost separates the classes, using a so-called soft margin. The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier.

9.2 Support Vector Classifiers

9.2.1 Overview of the Support Vector Classifier

In Figure 9.4, we see that observations that belong to two classes are not necessarily separable by a hyperplane. In fact, even if a separating hyper- plane does exist, then there are instances in which a classifier based on a separating hyperplane might not be desirable. A classifier based on a separating hyperplane will necessarily perfectly classify all of the training observations;this can lead to sensitivityto individual observations.An ex- ample is shown in Figure 9.5. The addition of a single observation in the right-hand panel of Figure 9.5 leads to a dramatic change in the maxi- mal margin hyperplane. The resulting maximal margin hyperplane is not satisfactory—for one thing, it has only a tiny margin. This is problematic because as discussed previously, the distance of an observation from the hyperplane can be seen as a measure of our confidence that the obser- vation was correctly classified. Moreover, the fact that the maximal mar- gin hyperplane is extremely sensitive to a change in a single observation suggests that it may have overfit the training data.

In this case, we might be willing to consider a classifier based on a hy- perplane that does not perfectly separate the two classes, in the interest of

374 9. Support Vector Machines

−1 0 1 2 3

3

2

1

0

1−

−1 0 1 2 3

3

2

1

0

1−

X X

1 1

X 2 X 2

FIGURE 9.5. Left: Two classes of observations are shown in blue and in purple, along with the maximal margin hyperplane. Right: An additional blue observation has been added, leading to a dramatic shift in the maximal margin hyperplane shown as a solid line. The dashed line indicates the maximal margin hyperplane that was obtained in the absence of this additional point.

• Greater robustness to individual observations, and

• Better classification of most of the training observations.

That is, it could be worthwhile to misclassify a few training observations in order to do a better job in classifying the remaining observations.

The support vector classifier, sometimes called a soft margin classifier, support does exactly this. Rather than seeking the largest possible margin so that vector every observation is not only on the correct side of the hyperplane but classifier also on the correct side of the margin, we instead allow some observations softmargin to be on the incorrect side of the margin, or even the incorrect side of classifier the hyperplane. (The margin is soft because it can be violated by some of the training observations.) An example is shown in the left-hand panel of Figure 9.6.Mostoftheobservationsareonthecorrectsideofthemargin.

However, a small subset of the observations are on the wrong side of the margin.

An observation can be not only on the wrong side of the margin,but also on the wrong side of the hyperplane. In fact, when there is no separating hyperplane,such a situation is inevitable.Observationsonthe wrong side of the hyper plane correspond to training observations that are misclassified by the support vector classifier. The right-hand panel of Figure 9.6 illustrates such a scenario.

9.2.2 Details of the Support Vector Classifier

The support vector classifier classifies a test observation depending on which side of a hyperplane it lies. The hyperplane is chosen to correctly separate most of the training observations into the two classes, but may

9.2 Support Vector Classifiers 375

−0.5 0.0 0.5 1.0 1.5 2.0 2.5

4

3

2

1

0

1−

10 7

9

8

1

3

4 5 2

6

−0.5 0.0 0.5 1.0 1.5 2.0 2.5

4

3

2

1

0

1−

10 7

11

9

8

1

12

3

4 5 2

6

X X

1 1

X 2 X 2

FIGURE 9.6. Left: A support vector classifier was fit to a small data set. The hyperplane is shown as a solid line and the margins are shown as dashed lines.

Purple observations: Observations 3,4,5, and 6 are on the correct side of the margin,observation 2 isonthemargin,andobservation 1 isonthewrongsideof the margin. Blue observations: Observations 7 and 10 are on the correct side of themargin,observation 9 isonthemargin,andobservation 8 isonthewrongside of the margin. No observations are on the wrong side of the hyperplane. Right:

Same as left panel with two additional points, 11 and 12. These two observations are on the wrong side of the hyperplane and the wrong side of the margin.

misclassifyafewobservations.Itisthesolutiontotheoptimizationproblem maximize M (9.12)

β0,β1,...,βp,&1,...,&n,M p subject to β2 =1, (9.13) j j=1

0 y (β +β x +β x + +β x ) M(1 " ), (9.14) i 0 1 i 1 2 i 2 p ip i

··· ≥ − n

" 0, " C, (9.15) i i

≥ ≤ i=1

0 where C is a nonnegative tuning parameter. As in (9.11), M is the width ofthemargin;weseektomakethisquantityaslargeaspossible.In(9.14),

" ,...," are slack variables that allow individual observations to be on

1 n slack the wrong side of the margin or the hyperplane; we will explain them in variable greater detail momentarily. Once we have solved (9.12)–(9.15), we classify atestobservationx asbefore,bysimplydeterminingonwhichsideofthe

∗ hyperplane it lies. That is, we classify the test observation based on the sign of f(x )=β +β x + +β x .

∗ 0 1 ∗1

··· p ∗p

The problem (9.12)–(9.15) seems complex, but insight into its behavior canbemadethroughaseriesofsimpleobservationspresentedbelow.First of all, the slack variable " tells us where the ith observation is located, i relativetothehyperplaneandrelativetothemargin.If" =0 thentheith i observationisonthecorrectsideofthemargin,aswesawin Section 9.1.4.

If " > 0 then the ith observation is on the wrong side of the margin, and i we say that the ith observation has violated the margin. If " >1 then it is i on the wrong side of the hyperplane.

376 9. Support Vector Machines

Wenowconsidertheroleofthetuningparameter C.In(9.15),C bounds thesumofthe" ’s,andsoitdeterminesthenumberandseverityofthevio- i lationstothemargin(andtothehyperplane)thatwewilltolerate.Wecan think of C as a budget for the amount that the margin can be violated by the n observations. If C = 0 then there is no budget for violations to the margin, and it must be the case that " = = " = 0, in which case

1 n

···

(9.12)–(9.15)simplyamountstothemaximalmarginhyperplaneoptimiza- tionproblem(9.9)–(9.11).(Ofcourse,amaximalmarginhyperplaneexists only if the two classes are separable.) For C >0 no more than C observa- tionscanbeonthewrongsideofthehyperplane,becauseifanobservation is on the wrong side of the hyperplane then " > 1, and (9.15) requires i that n " C. As the budget C increases, we become more tolerant of i=1 i ≤ violations to the margin, and so the margin will widen. Conversely, as C

) decreases, we become less tolerant of violations to the margin and so the margin narrows. An example is shown in Figure 9.7.

Inpractice,C is treated as a tuning parameter that is generallychosenvia cross-validation.As with the tuning parameters that we have seen through- outthisbook,C controlsthebias-variance trade-off of the statistical learning technique. When C is small, we seek narrow margins that are rarely violated; this amounts to a classifier that is highly fit to the data, which may have low bias but high variance.On the otherhand,when C is larger, the margin is wider and we allow more violations to it; this amounts to fitting the dataless hard and obtaining a classifier that is potentially more biased but may have lower variance.

The optimization problem (9.12)–(9.15) has a very interesting property: it turns out that only observations that either lie on the margin or that violate the margin will affect the hyperplane, and hence the classifier ob- tained. In other words, an observation that lies strictly on the correct side of the margin does not affect the support vector classifier! Changing the positionofthatobservationwouldnotchangetheclassifieratall,provided that its position remains on the correct side of the margin. Observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. These observations do affect the support vector classifier.

The fact that only support vectors affect the classifier is in line with our previousassertionthat C controlsthebias-variancetrade-offofthesupport vectorclassifier.Whenthetuningparameter C islarge,thenthemarginis wide,manyobservationsviolatethemargin,andsotherearemanysupport vectors. In this case, many observations are involved in determining the hyperplane. The top left panel in Figure 9.7 illustrates this setting: this classifier has low variance (since many observations are support vectors) butpotentiallyhighbias.Incontrast,if C issmall,then there will be fewer support vectors and hence the resulting classifier will have low bias but highvariance.Thebottomrightpanelin Figure 9.7 illustratesthissetting, with only eight support vectors.

The fact that the support vector classifier’s decision rule is based only onapotentiallysmallsubsetofthetrainingobservations(thesupportvec- tors) means that it is quite robust to the behavior of observations that are far away from the hyperplane. This property is distinct from some of

9.3 Support Vector Machines 377

−1 0 1 2

3

2

1

0

1−

2−

3−

−1 0 1 2

3

2

1

0

1−

2−

3−

−1 0 1 2

3

2

1

0

1−

2−

3−

−1 0 1 2

3

2

1

0

1−

2−

3−

X X

1 1

X X

1 1

X

X

2

2

X

X

2

2

FIGURE 9.7. A support vector classifier was fit using four different values of the tuning parameter C in (9.12)–(9.15). The largest value of C was used in the top left panel, and smaller values were used in the top right, bottom left, and bottom right panels. When C is large, then there is a high tolerance for observations being on the wrong side of the margin, and so the margin will be large. As C decreases, the tolerance for observations being on the wrong side of the margin decreases, and the margin narrows.

the other classification methods that we have seen in preceding chapters, suchaslineardiscriminantanalysis.Recallthatthe LDAclassificationrule dependsonthemeanofall oftheobservationswithineachclass,aswellas the within-class covariance matrix computed using all of the observations.

In contrast, logistic regression, unlike LDA, has very low sensitivity to ob- servationsfarfromthedecisionboundary.Infactwewillseein Section 9.5 thatthesupportvectorclassifierandlogisticregressionarecloselyrelated.

9.3 Support Vector Machines

We first discuss a general mechanism for converting a linear classifier into one that produces non-linear decision boundaries. We then introduce the support vector machine, which does this in an automatic way.

378 9. Support Vector Machines

−4 −2 0 2 4

4

2

0

2−

4−

−4 −2 0 2 4

4

2

0

2−

4−

X X

1 1

X 2 X 2

FIGURE 9.8. Left: The observations fall into two classes, with a non-lin- ear boundary between them. Right: The support vector classifier seeks a linear boundary, and consequently performs very poorly.

9.3.1 Classification with Non-Linear Decision Boundaries

The support vector classifier is a natural approach for classification in the two-class setting, if the boundary between the two classes is linear. How- ever, in practice we are sometimes faced with non-linear class boundaries.

For instance, consider the data in the left-hand panel of Figure 9.8. It is clear that a support vector classifier or any linear classifier will perform poorly here. Indeed, the support vector classifier shown in the right-hand panel of Figure 9.8 is useless here.

In Chapter 7, we are faced with an analogous situation. We see there that the performance of linear regression can suffer when there is a non- linear relationship between the predictors and the outcome. In that case, we consider enlarging the feature space using functions of the predictors, such as quadratic and cubic terms, in order to address this non-linearity.

In the case of the support vector classifier, we could address the prob- lem of possibly non-linear boundaries between classes in a similar way, by enlarging the feature space using quadratic, cubic, and even higher-order polynomial functions of the predictors. For instance, rather than fitting a support vector classifier using p features

X ,X ,...,X ,

1 2 p we could instead fit a support vector classifier using 2 p features

X ,X2,X ,X2,...,X ,X2.

1 1 2 2 p p

9.3 Support Vector Machines 379

Then (9.12)–(9.15) would become maximize M (9.16)

β0,β11,β12,...,βp 1,βp 2,&1,...,&n,M p p subject to y β + β x + β x 2 M(1 " ), i  0 j 1 ij j 2 ij≥ − i j=1 j=1

0 0 n  p 2 

" C, " 0, β2 =1.

i ≤ i ≥ jk i=1 j=1 k=1

0 00

Why does this lead to a non-linear decision boundary? In the enlarged feature space, the decision boundary that results from (9.16) is in fact lin- ear. But in the original feature space, the decision boundary is of the form q(x) = 0, where q is a quadratic polynomial, and its solutions are gener- ally non-linear. One might additionally want to enlarge the feature space with higher-order polynomial terms, or with interaction terms of the form

X X for j = j . Alternatively, other functions of the predictors could j j!

%

$ be considered rather than polynomials. It is not hard to see that there are many possible ways to enlarge the feature space, and that unless we are careful, we could end up with a huge number of features. Then compu- tations would become unmanageable. The support vector machine, which wepresentnext,allowsustoenlargethefeaturespaceusedbythesupport vector classifier in a way that leads to efficient computations.

9.3.2 The Support Vector Machine

The support vector machine (SVM) is an extension of the support vector support classifier that results from enlarging the feature space in a specific way, vector using kernels. We will now discuss this extension, the details of which are machine somewhat complex and beyond the scope of this book. However, the main kernel ideaisdescribedin Section 9.3.1:we may want to enlarge our feature space in order to accommodate a non-linear boundary between the classes. The kernel approach that we describe here is simply an efficient computational approach for enacting this idea.

We have not discussed exactly how the support vector classifier is com- puted because the details become somewhat technical. However, it turns out that the solution to the support vector classifier problem (9.12)–(9.15) involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two r-vectors a and b is defined as a,b = r a b . Thus the inner product of two observations

4 5 i=1 i i x , x is given by i i!

) p x ,x = x x . (9.17)

4 i i!5 ij i!j j=1

0

It can be shown that

• The linear support vector classifier can be represented as n f(x)=β + α x,x , (9.18)

0 i i

4 5 i=1

0

380 9. Support Vector Machines where there are n parameters α , i = 1,...,n, one per training i observation.

• To estimate the parameters α ,...,α and β , all we need are the

1 n 0 n innerproducts x ,x betweenallpairsoftrainingobservations.

2 4 i i!5

(The notation n means n(n 1)/2, and gives the number of pairs

’ ( 2 − among a set of n items.)

’ (

Notice that in (9.18), in order to evaluate the function f(x), we need to compute the inner product between the new point x and each of the training points x . However, it turns out that α is nonzero only for the support i i vectors in the solution—that is, if a training observation is not a support vector, then its α equals zero. So if is the collection of indices of these i

S support points, we can rewrite any solution function of the form (9.18) as f(x)=β + α x,x , (9.19)

0 i i

4 5 i

0∈S which typically involves far fewer terms than in (9.18).2

Tosummarize,in representing the linear classifierf(x),and in computing its coefficients, all we need are inner products.

Now suppose that every time the inner product (9.17) appears in the representation (9.18), or in a calculation of the solution for the support vector classifier, we replace it with a generalization of the inner product of the form

K(x ,x ), (9.20) i i!

where K is some function that we will refer to as a kernel. A kernel is a kernel functionthatquantifiesthesimilarityoftwoobservations.Forinstance,we could simply take p

K(x ,x )= x x , (9.21) i i! ij i!j j=1

0 which would just give us back the support vector classifier. Equation 9.21 is known as a linear kernel because the support vector classifier is linear in the features; the linear kernel essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation. But one could instead choose another form for (9.20). For instance, one could replace every instance of p x x with the quantity j=1 ij i!j

) p

K(x ,x )=(1+ x x )d. (9.22) i i! ij i!j j=1

0

This is known as a polynomial kernel of degree d, where d is a positive polynomial integer. Using such a kernel with d > 1, instead of the standard linear kernel kernel(9.21),inthesupportvectorclassifieralgorithmleadstoamuchmore flexibledecisionboundary.Itessentiallyamountstofittingasupportvector

2By expanding each of the inner products in (9.19), it is easy to see that f(x) is a linear function of the coordinates of x. Doing so also establishes the correspondence betweentheαi andtheoriginalparametersβj.

9.3 Support Vector Machines 381

−4 −2 0 2 4

4

2

0

2−

4−

−4 −2 0 2 4

4

2

0

2−

4−

X X

1 1

X 2 X 2

FIGURE 9.9. Left: An SVM with a polynomial kernel of degree 3 is applied to the non-linear data from Figure 9.8, resulting in a far more appropriate decision rule.Right:An SVMwitharadialkernelisapplied.Inthisexample,eitherkernel is capable of capturing the decision boundary.

classifier in a higher-dimensional space involving polynomials of degree d, rather than in the original feature space.When the support vector classifier is combined with a non-linearkernelsuchas(9.22),theresultingclassifieris known as a support vector machine.Note that in this case the(non-linear) function has the form f(x)=β + α K(x,x ). (9.23)

0 i i i

0∈S

The left-hand panel of Figure 9.9 shows an example of an SVM with a polynomial kernel applied to the non-linear data from Figure 9.8.The fit is a substantial improvement over the linear support vector classifier. When d=1,thenthe SVMreducestothesupportvectorclassifierseenearlierin this chapter.

The polynomial kernel shown in (9.22) is one example of a possible non-linear kernel, but alternatives abound. Another popular choice is the radial kernel, which takes the form radialkernel p

K(x ,x )=exp( γ (x x )2). (9.24) i i!

− ij

− i!j j=1

0

In(9.24),γ is a positive constant.Th eright-hand panel of Figure 9.9 shows an example of an SVM with a radial kernel on this non-linear data; it also does a good job in separating the two classes.

How does the radial kernel (9.24) actually work? If a given test obser- vation x =(x ,...,x )T is far from a training observation x in terms of

∗ ∗1 ∗p i

Euclideandistance,then p (x x )2 willbelarge,andso K(x ,x )= j=1 ∗j− ij ∗ i exp( γ p (x x )2) will be tiny. This means that in (9.23), x will

− j=1 ∗j − ij ) i play virtually no role in f(x ). Recall that the predicted class label for the

∗

) test observation x is based on the sign of f(x ). In other words, training

∗ ∗ observations that are far from x will play essentially no role in the pre-

∗ dicted class label for x . This means that the radial kernel has very local

∗

382 9. Support Vector Machines

False positive rate etar evitisop eur T

0.0 0.2 0.4 0.6 0.8 1.0

0.1

8.0

6.0

4.0

2.0

0.0

Support Vector Classifier

LDA

False positive rate etar evitisop eur T

0.0 0.2 0.4 0.6 0.8 1.0

0.1

8.0

6.0

4.0

2.0

0.0

Support Vector Classifier

SVM: γ=10−3

SVM: γ=10−2

SVM: γ=10−1

FIGURE 9.10. ROC curves for the Heart data training set. Left: The support vector classifier and LDA are compared. Right: The support vector classifier is compared to an SVM using a radial basis kernel with γ =10− 3, 10− 2, and 10− 1.

behavior,inthesensethatonlynearbytrainingobservationshaveaneffect on the class label of a test observation.

What is the advantage of using a kernel rather than simply enlarging the feature space using functions of the original features, as in (9.16)? One advantage is computational, and it amounts to the fact that using kernels, oneneedonlycompute K(x ,x )forall n distinctpairsi, i.Thiscanbe i $i 2 $ done without explicitly working in the enlarged feature space. This is im-

’ ( portant because in many applications of SVMs, the enlarged feature space issolargethatcomputationsareintractable.Forsomekernels,suchasthe radial kernel (9.24), the feature space is implicit and infinite-dimensional, so we could never do the computations there anyway!

9.3.3 An Application to the Heart Disease Data

In Chapter 8 weapplydecisiontreesandrelatedmethodstothe Heartdata.

Theaimistouse 13 predictorssuchas Age,Sex,and Cholinordertopredict whether an individual has heart disease. We now investigate how an SVM compares to LDA on this data. After removing 6 missing observations, the dataconsistof 297 subjects,whichwerandomlysplitinto 207 trainingand

90 test observations.

We first fit LDA and the support vector classifier to the training data.

Notethatthesupportvectorclassifierisequivalenttoan SVMusingapoly- nomial kernel of degree d=1. The left-hand panel of Figure 9.10 displays

ROCcurves(describedin Section 4.4.2)forthetrainingsetpredictionsfor both LDAandthesupportvectorclassifier.Bothclassifierscomputescores of the form fˆ(X) = βˆ +βˆ X +βˆ X + +βˆ X for each observation.

0 1 1 2 2 p p

···

For any given cutoff t, we classify observations into the heart disease or no heart disease categories depending on whether fˆ(X) < t or fˆ(X) t.

≥

The ROC curve is obtained by forming these predictions and computing thefalsepositiveandtruepositiveratesforarangeofvaluesoft.Anopti- mal classifier will hug the top left corner of the ROC plot. In this instance

9.4 SVMs with More than Two Classes 383

False positive rate etar evitisop eur T

0.0 0.2 0.4 0.6 0.8 1.0

0.1

8.0

6.0

4.0

2.0

0.0

Support Vector Classifier

LDA

False positive rate etar evitisop eur T

0.0 0.2 0.4 0.6 0.8 1.0

0.1

8.0

6.0

4.0

2.0

0.0

Support Vector Classifier

SVM: γ=10−3

SVM: γ=10−2

SVM: γ=10−1

FIGURE9.11. ROCcurvesforthetestsetofthe Heartdata.Left:Thesupport vector classifier and LDA are compared. Right: The support vector classifier is compared to an SVM using a radial basis kernel with γ =10− 3, 10− 2, and 10− 1.

LDA and the support vector classifier both perform well, though there is a suggestion that the support vector classifier may be slightly superior.

Theright-handpanelof Figure 9.10 displays ROCcurvesfor SVMsusing aradialkernel,withvariousvaluesof γ.Asγ increasesandthefitbecomes more non-linear, the ROC curves improve. Using γ =10 1 appears to give

− an almost perfect ROC curve. However, these curves represent training error rates, which can be misleading in terms of performance on new test data. Figure 9.11 displays ROC curves computed on the 90 test observa- tions. We observe some differences from the training ROC curves. In the left-handpanelof Figure 9.11,thesupportvectorclassifierappearstohave a small advantage over LDA (although these differences are not statisti- cally significant). In the right-hand panel, the SVM using γ =10 1, which

− showed the best results on the training data, produces the worst estimates on the test data. This is once again evidence that while a more flexible method will often produce lower training error rates, this does not neces- sarilyleadtoimprovedperformanceontestdata.The SVMswithγ =10 2

− and γ =10 3 perform comparably to the support vector classifier, and all

− three outperform the SVM with γ =10 1.

−

9.4 SVMs with More than Two Classes

So far, our discussion has been limited to the case of binary classification: that is, classification in the two-class setting. How can we extend SVMs to the more general case where we have some arbitrary number of classes?

It turns out that the concept of separating hyperplanes upon which SVMs are based does not lend itself naturally to more than two classes. Though a number of proposals for extending SVMs to the K-class case have been made, the two most popular are the one-versus-one and one-versus-all approaches. We briefly discuss those two approaches here.

384 9. Support Vector Machines

9.4.1 One-Versus-One Classification

Supposethatwewouldliketoperformclassificationusing SVMs,andthere are K > 2 classes. A one-versus-one or all-pairs approach constructs K

2 one-versus-

SVMs, each of which compares a pair of classes. For example, one such one

’ (

SVM might compare the kth class, coded as +1, to the k th class, coded

$ as 1. We classify a test observation using each of the K classifiers, and

− 2 we tally the number of times that the test observation is assigned to each

’ ( of the K classes. The final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these

K pairwise classifications.

2

’ (

9.4.2 One-Versus-All Classification

The one-versus-all approach (also referred to as one-versus-rest) is an al- one-versus- ternative procedure for applying SVMs in the case of K > 2 classes. We all fit K SVMs, each time comparing one of the K classes to the remaining one-versus-

K 1 classes. Let β ,β ,...,β denote the parameters that result from rest

0 k 1 k pk

− fittingan SVMcomparingthekthclass(codedas+1)totheothers(coded as 1). Let x denote a test observation. We assign the observation to the

∗

− classforwhichβ +β x +β x + +β x islargest,asthisamounts

0 k 1 k ∗1 2 k ∗2

··· pk ∗p to a high level of confidence that the test observation belongs to the kth class rather than to any of the other classes.

9.5 Relationship to Logistic Regression

When SVMs were first introduced in the mid-1990 s, they made quite a splash in the statistical and machine learning communities. This was due in part to their good performance, good marketing, and also to the fact that the underlying approach seemed both novel and mysterious. The idea offindingahyperplanethatseparatesthedataaswellaspossible,whileal- lowing some violations to this separation, seemed distinctly different from classical approaches for classification, such as logistic regression and lin- ear discriminant analysis. Moreover, the idea of using a kernel to expand the feature space in order to accommodate non-linear class boundaries ap- peared to be a unique and valuable characteristic.

However, since that time, deep connections between SVMs and other more classical statistical methods have emerged. It turns out that one can rewrite the criterion (9.12)–(9.15) for fitting the support vector classifier f(X)=β +β X + +β X as

0 1 1 p p

··· n p minimize max[0,1 y f(x )]+λ β2 , (9.25)

β0,β1,...,βp 

0 i=1

− i i

0 j=1 j



 whereλisanonnegativetuningparameter.Whenλislargethenβ

1

,...,β p are small, more violations to the margin are tolerated, and a low-variance but high-bias classifier will result. When λ is small then few violations to the margin will occur; this amounts to a high-variance but low-bias

9.5 Relationship to Logistic Regression 385 classifier. Thus, a small value of λ in (9.25) amounts to a small value of C in (9.15). Note that the λ p β2 term in (9.25) is the ridge penalty term j=1 j from Section 6.2.1, and plays a similar role in controlling the bias-variance

) trade-off for the support vector classifier.

Now(9.25)takesthe“Loss+Penalty”formthatwehaveseenrepeatedly throughout this book: minimize L(X,y,β)+λP(β) . (9.26)

β0,β1,...,βp { }

In (9.26), L(X,y,β) is some loss function quantifying the extent to which the model, parametrized by β, fits the data (X,y), and P(β) is a penalty function on the parameter vector β whose effect is controlled by a nonneg- ativetuningparameterλ.Forinstance,ridgeregressionandthelassoboth take this form with

2 n p

L(X,y,β)= y β x β i 0 ij j

 − −  i=1 j=1

0 0

  and with P(β) = p β2 for ridge regression and P(β) = p β for j=1 j j=1| j | the lasso. In the case of (9.25) the loss function instead takes the form

) ) n

L(X,y,β)= max[0,1 y (β +β x + +β x )].

i 0 1 i 1 p ip

− ··· i=1

0

This is known as hinge loss, and is depicted in Figure 9.12. However, it hingeloss turns out that the hinge loss function is closely related to the loss function used in logistic regression, also shown in Figure 9.12.

An interesting characteristic of the support vector classifier is that only support vectors play a role in the classifier obtained; observations on the correct side of the margin do not affect it. This is due to the fact that the lossfunctionshownin Figure 9.12 isexactlyzeroforobservationsforwhich y (β +β x + +β x ) 1; these correspond to observations that are i 0 1 i 1 p ip

··· ≥ onthecorrectsideofthemargin.3 Incontrast,thelossfunctionforlogistic regressionshownin Figure 9.12 isnotexactlyzeroanywhere.Butitisvery small for observations that are far from the decision boundary. Due to the similaritiesbetweentheirlossfunctions,logisticregressionandthesupport vector classifier often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.

Whenthesupportvectorclassifierand SVMwerefirstintroduced,itwas thought that the tuning parameter C in (9.15) was an unimportant “nui- sance” parameter that could be set to some default value, like 1. However, the “Loss + Penalty” formulation (9.25) for the support vector classifier indicates that this is not the case. The choice of tuning parameter is very importantanddeterminestheextenttowhichthemodelunderfitsorover- fits the data, as illustrated, for example, in Figure 9.7.

3Withthishinge-loss+penaltyrepresentation,themargincorrespondstothevalue one,andthewidthofthemarginisdeterminedby β2.

j

!

386 9. Support Vector Machines

−6 −4 −2 0 2 sso L

8

6

4

2

0

SVM Loss

Logistic Regression Loss y i(β0+β1 x i 1+...+β p x ip)

FIGURE 9.12. The SVM and logistic regression loss functions are compared, asafunctionofy i

(β

0

+β

1 x i 1

+ +β p x ip

).Wheny i

(β

0

+β

1 x i 1

+ +β p x ip

)is

··· ··· greaterthan 1,thenthe SVMlossiszero,sincethiscorrespondstoanobservation thatisonthecorrectsideofthemargin.Overall,thetwolossfunctionshavequite similar behavior.

We have established that the support vector classifier is closely related tologisticregressionandotherpreexistingstatisticalmethods.Isthe SVM unique in its use of kernels to enlarge the feature space to accommodate non-linear class boundaries? The answer to this question is “no”. We could just as well perform logistic regression or many of the other classification methods seen in this book using non-linear kernels; this is closely related to some of the non-linear approaches seen in Chapter 7. However, for his- torical reasons, the use of non-linear kernels is much more widespread in the context of SVMs than in the context of logistic regression or other methods.

Though we have not addressed it here, there is in fact an extension of the SVM for regression (i.e. for a quantitative rather than a qualita- tive response), called support vector regression. In Chapter 3, we saw that support least squares regression seeks coefficients β ,β ,...,β such that the sum

0 1 p vector of squared residuals is as small as possible. (Recall from Chapter 3 that regression residuals are defined as y β β x β x .) Support vector i 0 1 i 1 p ip

− − − ··· − regression instead seeks coefficients that minimize a different type of loss, where only residuals larger in absolute value than some positive constant contribute to the loss function. This is an extension of the margin used in support vector classifiers to the regression setting.

9.6 Lab: Support Vector Machines 387

9.6 Lab: Support Vector Machines

In this lab, we use the sklearn.svm library to demonstrate the support vector classifier and the support vector machine.

We import some of our usual libraries.

In[1]: import numpy as np from matplotlib.pyplot import subplots, cm import sklearn.model_selection as skm from ISLP import load_data, confusion_table

We also collect the new imports needed for this lab.

In[2]: from sklearn.svm import SVC from ISLP.svm import plot as plot_svm from sklearn.metrics import Roc Curve Display

We will use the function Roc Curve Display.from_estimator() to produce

Roc Curve several ROC plots, using a shorthand roc_curve.

Display.from_ estimator()

In[3]: roc_curve = Roc Curve Display.from_estimator # shorthand

9.6.1 Support Vector Classifier

We now use the Support Vector Classifier() function (abbreviated SVC())

Support Vector from sklearn to fit the support vector classifier for a given value of the

Classifier() parameter C. The C argument allows us to specify the cost of a violation to the margin. When the cost argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin.Whenthe Cargumentislarge,thenthemarginswillbenarrowand there will be few support vectors on the margin or violating the margin.

Here we demonstrate the use of SVC() on a two-dimensional example, so that we can plot the resulting decision boundary. We begin by generating the observations, which belong to two classes, and checking whether the classes are linearly separable.

In[4]: rng = np.random.default_rng(1)

X = rng.standard_normal((50, 2)) y = np.array([-1]*25+[1]*25)

X[y==1] += 1 fig, ax = subplots(figsize=(8,8)) ax.scatter(X[:,0],

X[:,1], c=y, cmap=cm.coolwarm);

They are not. We now fit the classifier.

In[5]: svm_linear = SVC(C=10, kernel='linear') svm_linear.fit(X, y)

Out[5]:SVC(C=10, kernel='linear')

Thesupportvectorclassifierwithtwofeaturescanbevisualizedbyplot- ting values of its decision function. We have included a function for this in decision the ISLP package (inspired by a similar example in the sklearn docs).

function

388 9. Support Vector Machines

In[6]: fig, ax = subplots(figsize=(8,8)) plot_svm(X, y, svm_linear, ax=ax)

The decision boundary between the two classes is linear (because we usedtheargumentkernel='linear').Thesupportvectorsaremarkedwith

+ and the remaining observations are plotted as circles.

What if we instead used a smaller value of the cost parameter?

In[7]: svm_linear_small = SVC(C=0.1, kernel='linear') svm_linear_small.fit(X, y) fig, ax = subplots(figsize=(8,8)) plot_svm(X, y, svm_linear_small, ax=ax)

With a smaller value of the cost parameter, we obtain a larger number of support vectors, because the margin is now wider. For linear kernels, we can extract the coefficients of the linear decision boundary as follows:

In[8]: svm_linear.coef_

Out[8]:array([[1.173 , 0.7734]])

Since the support vector machine is an estimator in sklearn, we can use the usual machinery to tune it.

In[9]: kfold = skm.KFold(5, random_state=0, shuffle=True) grid = skm.Grid Search CV(svm_linear,

{'C':[0.001,0.01,0.1,1,5,10,100]}, refit=True, cv=kfold, scoring='accuracy') grid.fit(X, y) grid.best_params_

Out[9]:{'C': 1}

Wecaneasilyaccessthecross-validationerrorsforeachofthesemodelsin grid.cv_results_.Thisprintsoutalotofdetail,soweextracttheaccuracy results only.

In[10]: grid.cv_results_[('mean_test_score')]

Out[10]:array([0.46, 0.46, 0.72, 0.74, 0.74, 0.74, 0.74])

We see that C=1 results in the highest cross-validation accuracy of 0.74, though the accuracy is the same for several values of C. The classifier grid.best_estimator_ can be used to predict the class label on a set of test observations. Let’s generate a test data set.

9.6 Lab: Support Vector Machines 389

In[11]: X_test = rng.standard_normal((20, 2)) y_test = np.array([-1]*10+[1]*10)

X_test[y_test==1] += 1

Nowwepredicttheclasslabelsofthesetestobservations.Hereweusethe best model selected by cross-validation in order to make the predictions.

In[12]: best_ = grid.best_estimator_ y_test_hat = best_.predict(X_test) confusion_table(y_test_hat, y_test)

Out[12]: Truth -1 1

Predicted

-1 8 4

1 2 6

Thus, with this value of C, 70% of the test observations are correctly clas- sified. What if we had instead used C=0.001?

In[13]: svm_ = SVC(C=0.001, kernel='linear').fit(X, y) y_test_hat = svm_.predict(X_test) confusion_table(y_test_hat, y_test)

Out[13]: Truth -1 1

Predicted

-1 2 0

1 8 10

In this case 60% of test observations are correctly classified.

We now consider a situation in which the two classes are linearly sepa- rable. Then we can find an optimal separating hyperplane using the SVC() estimator. We first further separate the two classes in our simulated data so that they are linearly separable:

In[14]: X[y==1] += 1.9; fig, ax = subplots(figsize=(8,8)) ax.scatter(X[:,0], X[:,1], c=y, cmap=cm.coolwarm);

Now the observations are just barely linearly separable.

In[15]: svm_ = SVC(C=1 e 5, kernel='linear').fit(X, y) y_hat = svm_.predict(X) confusion_table(y_hat, y)

Out[15]: Truth -1 1

Predicted

-1 25 0

1 0 25

Wefitthesupportvectorclassifierandplottheresultinghyperplane,using a very large value of C so that no observations are misclassified.

In[16]: fig, ax = subplots(figsize=(8,8)) plot_svm(X, y, svm_, ax=ax)

390 9. Support Vector Machines

Indeed no training errors were made and only three support vectors were used.Infact,thelargevalueof Calsomeansthatthesethreesupportpoints are on the margin, and define it. One may wonder how good the classifier could be on test data that depends on only three data points! We now try a smaller value of C.

In[17]: svm_ = SVC(C=0.1, kernel='linear').fit(X, y) y_hat = svm_.predict(X) confusion_table(y_hat, y)

Out[17]: Truth -1 1

Predicted

-1 25 0

1 0 25

Using C=0.1, we again do not misclassify any training observations, but we also obtain a much wider margin and make use of twelve support vectors.

These jointly define the orientation of the decision boundary, and since there are more of them, it is more stable. It seems possible that this model will perform better on test data than the model with C=1 e 5 (and indeed, a simple experiment with a large test set would bear this out).

In[18]: fig, ax = subplots(figsize=(8,8)) plot_svm(X, y, svm_, ax=ax)

9.6.2 Support Vector Machine

In order to fit an SVM using a non-linear kernel, we once again use the

SVC() estimator. However, now we use a different value of the parameter kernel.Tofitan SVMwithapolynomialkernelweusekernel="poly",and to fit an SVM with a radial kernel we use kernel="rbf". In the former case we also use the degree argument to specify a degree for the polynomial kernel (this is d in (9.22)), and in the latter case we use gamma to specify a value of γ for the radial basis kernel (9.24).

Wefirstgeneratesomedatawithanon-linearclassboundary,asfollows:

In[19]: X = rng.standard_normal((200, 2))

X[:100] += 2

X[100:150] -= 2 y = np.array([1]*150+[2]*50)

Plotting the data makes it clear that the class boundary is indeed non- linear.

In[20]: fig, ax = subplots(figsize=(8,8)) ax.scatter(X[:,0],

X[:,1], c=y, cmap=cm.coolwarm)

Out[20]:<matplotlib.collections.Path Collection at 0 x 7 faa 9 ba 52 eb 0>

9.6 Lab: Support Vector Machines 391

The data is randomly split into training and testing groups. We then fit thetrainingdatausingthe SVC()estimatorwitharadialkernelandγ =1:

In[21]: (X_train,

X_test, y_train, y_test) = skm.train_test_split(X, y, test_size=0.5, random_state=0) svm_rbf = SVC(kernel="rbf", gamma=1, C=1) svm_rbf.fit(X_train, y_train)

Theplotshowsthattheresulting SVMhasadecidedlynon-linearbound- ary.

In[22]: fig, ax = subplots(figsize=(8,8)) plot_svm(X_train, y_train, svm_rbf, ax=ax)

Wecanseefromthefigurethatthereareafairnumberoftrainingerrors in this SVM fit. If we increase the value of C, we can reduce the number of training errors. However, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data.

In[23]: svm_rbf = SVC(kernel="rbf", gamma=1, C=1 e 5) svm_rbf.fit(X_train, y_train) fig, ax = subplots(figsize=(8,8)) plot_svm(X_train, y_train, svm_rbf, ax=ax)

We can perform cross-validation using skm.Grid Search CV() to select the best choice of γ and C for an SVM with a radial kernel:

In[24]: kfold = skm.KFold(5, random_state=0, shuffle=True) grid = skm.Grid Search CV(svm_rbf,

{'C':[0.1,1,10,100,1000],

'gamma':[0.5,1,2,3,4]}, refit=True, cv=kfold, scoring='accuracy'); grid.fit(X_train, y_train) grid.best_params_

Out[24]:{'C': 100, 'gamma': 1}

The best choice of parameters under five-fold CV is achieved at C=1 and gamma=0.5, though several other values also achieve the same value.

In[25]: best_svm = grid.best_estimator_ fig, ax = subplots(figsize=(8,8)) plot_svm(X_train,

392 9. Support Vector Machines y_train, best_svm, ax=ax) y_hat_test = best_svm.predict(X_test) confusion_table(y_hat_test, y_test)

Out[25]: Truth 1 2

Predicted

1 69 6

2 6 19

With these parameters, 12% of test observations are misclassified by this

SVM.

9.6.3 ROC Curves

SVMsandsupportvectorclassifiersoutputclasslabelsforeachobservation.

However, it is also possible to obtain fitted values for each observation, whicharethenumericalscoresusedtoobtaintheclasslabels.Forinstance, inthecaseofasupportvectorclassifier,thefittedvalueforanobservation

X =(X ,X ,...,X )T takestheformβˆ +βˆ X +βˆ X +...+βˆ X .For

1 2 p 0 1 1 2 2 p p an SVM with a non-linear kernel, the equation that yields the fitted value is given in (9.23). The sign of the fitted value determines on which side of the decision boundary the observation lies. Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero then it is assigned to the other.

By changing this threshold from zero to some positive value, we skew the classificationsinfavorofoneclassversustheother.Byconsideringarange ofthesethresholds,positiveandnegative,weproducetheingredientsfora

ROC plot. We can access these values by calling the decision_function()

.function_ method of a fitted SVM estimator.

decision()

The function ROCCurve Display.from_estimator() (which we have abbre- viatedtoroc_curve())willproduceaplotofa ROCcurve.Ittakesafitted roc_curve() estimatorasitsfirstargument,followedbyamodelmatrix X andlabelsy.

The argument name is used in the legend, while color is used for the color of the line. Results are plotted on our axis object ax.

In[26]: fig, ax = subplots(figsize=(8,8)) roc_curve(best_svm,

X_train, y_train, name='Training', color='r', ax=ax);

In this example, the SVM appears to provide accurate predictions. By increasing γ we can produce a more flexible fit and generate further im- provements in accuracy.

In[27]: svm_flex = SVC(kernel="rbf", gamma=50,

9.6 Lab: Support Vector Machines 393

C=1) svm_flex.fit(X_train, y_train) fig, ax = subplots(figsize=(8,8)) roc_curve(svm_flex,

X_train, y_train, name='Training $\gamma=50$', color='r', ax=ax);

However, these ROC curves are all on the training data. We are really more interested in the level of prediction accuracy on the test data. When we compute the ROC curves on the test data, the model with γ = 0.5 appears to provide the most accurate results.

In[28]: roc_curve(svm_flex,

X_test, y_test, name='Test $\gamma=50$', color='b', ax=ax) fig;

Let’s look at our tuned SVM.

In[29]: fig, ax = subplots(figsize=(8,8)) for (X_, y_, c, name) in zip(

(X_train, X_test),

(y_train, y_test),

('r', 'b'),

('CV tuned on training',

'CV tuned on test')): roc_curve(best_svm,

X_, y_, name=name, ax=ax, color=c)

9.6.4 SVM with Multiple Classes

If the response is a factor containing more than two levels, then the SVC() function will perform multi-class classification using either the one-versus- one approach (when decision_function_shape=='ovo') or one-versus-rest 4

(when decision_function_shape=='ovr'). We explore that setting briefly here by generating a third class of observations.

In[30]: rng = np.random.default_rng(123)

X = np.vstack([X, rng.standard_normal((50, 2))]) y = np.hstack([y, [0]*50])

X[y==0,1] += 2 fig, ax = subplots(figsize=(8,8)) ax.scatter(X[:,0], X[:,1], c=y, cmap=cm.coolwarm);

4One-versus-restisalsoknownasone-versus-all.

394 9. Support Vector Machines

We now fit an SVM to the data:

In[31]: svm_rbf_3 = SVC(kernel="rbf",

C=10, gamma=1, decision_function_shape='ovo'); svm_rbf_3.fit(X, y) fig, ax = subplots(figsize=(8,8)) plot_svm(X, y, svm_rbf_3, scatter_cmap=cm.tab 10, ax=ax)

The sklearn.svm library can also be used to perform support vector re- gression with a numerical response using the estimator Support Vector-

Regression().

Support Vector

Regression()

9.6.5 Application to Gene Expression Data

We now examine the Khan data set, which consists of a number of tissue samples corresponding to four distinct types of small round blue cell tu- mors. For each tissue sample, gene expression measurements are available.

Thedatasetconsistsoftrainingdata,xtrainandytrain,andtestingdata, xtest and ytest.

We examine the dimension of the data:

In[32]: Khan = load_data('Khan')

Khan['xtrain'].shape, Khan['xtest'].shape

Out[32]:((63, 2308), (20, 2308))

This data set consists of expression measurements for 2,308 genes. The training and test sets consist of 63 and 20 observations, respectively.

We will use a support vector approach to predict cancer subtype using geneexpressionmeasurements.Inthisdataset,thereisaverylargenumber of features relative to the number of observations. This suggests that we shouldusealinearkernel,becausetheadditionalflexibilitythatwillresult from using a polynomial or radial kernel is unnecessary.

In[33]: khan_linear = SVC(kernel='linear', C=10) khan_linear.fit(Khan['xtrain'], Khan['ytrain']) confusion_table(khan_linear.predict(Khan['xtrain']),

Khan['ytrain'])

Out[33]: Truth 1 2 3 4

Predicted

1 8 0 0 0

2 0 23 0 0

3 0 0 12 0

4 0 0 0 20

We see that there are no training errors. In fact, this is not surprising, becausethelargenumberofvariablesrelativetothenumberofobservations implies that it is easy to find hyperplanes that fully separate the classes.

9.7 Exercises 395

We are more interested in the support vector classifier’s performance on the test observations.

In[34]: confusion_table(khan_linear.predict(Khan['xtest']),

Khan['ytest'])

Out[34]: Truth 1 2 3 4

Predicted

1 3 0 0 0

2 0 6 2 0

3 0 0 4 0

4 0 0 0 5

We see that using C=10 yields two test set errors on these data.

9.7 Exercises

Conceptual

1. This problem involves hyperplanes in two dimensions.

(a) Sketch the hyperplane 1+3X X = 0. Indicate the set of

1 2

− points for which 1+3X X > 0, as well as the set of points

1 2

− for which 1+3X X <0.

1 2

−

(b) On the same plot, sketch the hyperplane 2+X +2X = 0.

1 2

−

Indicate the set of points for which 2+X +2X >0, as well

1 2

− as the set of points for which 2+X +2X <0.

1 2

−

2. We have seen that in p = 2 dimensions, a linear decision boundary takestheformβ +β X +β X =0.Wenowinvestigateanon-linear

0 1 1 2 2 decision boundary.

(a) Sketch the curve

(1+X )2+(2 X )2 =4.

1 2

−

(b) On your sketch, indicate the set of points for which

(1+X )2+(2 X )2 >4,

1 2

− as well as the set of points for which

(1+X )2+(2 X )2 4.

1 2

− ≤

(c) Supposethataclassifierassignsanobservationtotheblueclass if

(1+X )2+(2 X )2 >4,

1 2

− and to the red class otherwise. To what class is the observation

(0,0) classified? ( 1,1)? (2,2)? (3,8)?

−

(d) Argue that while the decision boundary in (c) is not linear in terms of X and X , it is linear in terms of X , X2, X , and

1 2 1 1 2

X2.

2

396 9. Support Vector Machines

3. Here we explore the maximal margin classifier on a toy data set.

(a) We are given n = 7 observations in p = 2 dimensions. For each observation, there is an associated class label.

Obs. X X Y

1 2

1 3 4 Red

2 2 2 Red

3 4 4 Red

4 1 4 Red

5 2 1 Blue

6 4 3 Blue

7 4 1 Blue

Sketch the observations.

(b) Sketchtheoptimalseparatinghyperplane,andprovidetheequa- tion for this hyperplane (of the form (9.1)).

(c) Describetheclassificationruleforthemaximalmarginclassifier.

It should be something along the lines of “Classify to Red if

β +β X +β X >0, and classify to Blue otherwise.” Provide

0 1 1 2 2 the values for β , β , and β .

0 1 2

(d) On your sketch, indicate the margin for the maximal margin hyperplane.

(e) Indicate the support vectors for the maximal margin classifier.

(f) Argue that a slight movement of the seventh observation would not affect the maximal margin hyperplane.

(g) Sketch a hyperplane that is not the optimal separating hyper- plane, and provide the equation for this hyperplane.

(h) Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane.

Applied

4. Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation be- tween the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the train- ing data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions.

5. Wehaveseenthatwecanfitan SVMwithanon-linearkernelinorder toperformclassificationusinganon-lineardecisionboundary.Wewill now see that we can also obtain a non-linear decision boundary by performinglogisticregressionusingnon-lineartransformationsofthe features.

9.7 Exercises 397

(a) Generateadatasetwithn=500 andp=2,suchthattheobser- vationsbelongtotwoclasseswithaquadraticdecisionboundary between them. For instance, you can do this as follows: rng = np.random.default_rng(5) x 1 = rng.uniform(size=500) - 0.5 x 2 = rng.uniform(size=500) - 0.5 y = x 1**2 - x 2**2 > 0

(b) Plot the observations, colored according to their class labels.

Your plot should display X on the x-axis, and X on the y-

1 2 axis.

(c) Fit a logistic regression model to the data, using X and X as

1 2 predictors.

(d) Apply this model to the training data in order to obtain a pre- dicted class label for each training observation. Plot the ob- servations, colored according to the predicted class labels. The decision boundary should be linear.

(e) Now fit a logistic regression model to the data using non-linear functionsof X and X aspredictors(e.g.X2,X X ,log(X ),

1 2 1 1 × 2 2 and so forth).

(f) Apply this model to the training data in order to obtain a pre- dicted class label for each training observation. Plot the ob- servations, colored according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, thenrepeat(a)–(e)untilyoucomeupwithanexampleinwhich the predicted class labels are obviously non-linear.

(g) Fit a support vector classifier to the data with X and X as

1 2 predictors. Obtain a class prediction for each training observa- tion. Plot the observations, colored according to the predicted class labels.

(h) Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.

(i) Comment on your results.

6. Attheendof Section 9.6.1,itisclaimedthatinthecaseofdatathatis justbarelylinearlyseparable, asupportvectorclassifierwitha small value of C that misclassifies a couple of training observations may performbetterontestdatathanonewithahugevalueof Cthatdoes not misclassify any training observations. You will now investigate this claim.

(a) Generatetwo-classdatawithp=2 insuchawaythattheclasses are just barely linearly separable.

(b) Compute the cross-validation error rates for support vector classifiers with a range of C values. How many training obser- vations are misclassified for each value of C considered, and how does this relate to the cross-validation errors obtained?

398 9. Support Vector Machines

(c) Generate an appropriate test data set, and compute the test errorscorrespondingtoeachofthevaluesof Cconsidered.Which value of C leads to the fewest test errors, and how does this compare to the values of C that yield the fewest training errors and the fewest cross-validation errors?

(d) Discuss your results.

7. In this problem, you will use support vector approaches in order to predictwhetheragivencargetshighorlowgasmileagebasedonthe

Auto data set.

(a) Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.

(b) Fit a support vector classifier to the data with various values of

C,inordertopredictwhetheracargetshighorlowgasmileage.

Report the cross-validation errors associated with different val- ues of this parameter. Comment on your results. Note you will needtofittheclassifierwithoutthegasmileagevariabletopro- duce sensible results.

(c) Now repeat (b), this time using SVMs with radial and polyno- mialbasiskernels,withdifferentvaluesofgammaanddegreeand

C. Comment on your results.

(d) Make some plots to back up your assertions in (b) and (c).

Hint:Inthelab,weusedtheplot_svm()functionforfitted SVMs.

When p > 2, you can use the keyword argument features to create plots displaying pairs of variables at a time.

8. This problem involves the OJ data set which is part of the ISLP package.

(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

(b) Fit a support vector classifier to the training data using

C = 0.01, with Purchase as the response and the other variables as predictors. How many support points are there?

(c) What are the training and test error rates?

(d) Use cross-validation to select an optimal C. Consider values in the range 0.01 to 10.

(e) Compute the training and test error rates using this new value for C.

(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.

(g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree = 2.

(h) Overall, which approach seems to give the best results on this data?